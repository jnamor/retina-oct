{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "massive-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "separate-gallery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utiliser cuda pour augmenter la rapidité d'exécution\n",
    "# GPU/graphics driver supports a particular version CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recognized-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for training and testing directory\n",
    "train_path = 'content/OCT2017/train/'\n",
    "test_path = 'content/OCT2017/test/'\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stylish-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((img_width, img_height)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # 0-1 to [-1, 1], formula (x-mean)/std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oriented-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform = transformer),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path, transform = transformer),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "matched-yacht",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNV', 'DME', 'DRUSEN', 'NORMAL']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categories\n",
    "root = pathlib.Path(train_path)\n",
    "classes = [file.name.split('/')[-1] for file in root.iterdir()]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "composed-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes = len(classes)):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        # Input shape= (256,3,150,150)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = 3, \n",
    "            out_channels = 12, \n",
    "            kernel_size = 3, \n",
    "            stride = 1, \n",
    "            padding = 1\n",
    "        )\n",
    "        # Shape = (256,12,150,150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 12)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Reduce the image size by factor of 2\n",
    "        # Shape = (256,12,75,75)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Shape = (256,20,75,75)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 12,\n",
    "            out_channels = 20,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Shape = (256,32,75,75)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels = 20,\n",
    "            out_channels = 32,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(num_features = 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            in_features = 75 * 75 * 32, \n",
    "            out_features = num_classes\n",
    "        )\n",
    "        \n",
    "    # Feed forward function\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "            \n",
    "        output = self.pool(output)\n",
    "            \n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "            \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "             \n",
    "            \n",
    "        # Above output will be in matrix form, with shape (256,32,75,75) \n",
    "        output = output.view(-1, 32*75*75)\n",
    "        output = self.fc(output)\n",
    "            \n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spiritual-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes = len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "verified-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valid-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tribal-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83484 968\n"
     ]
    }
   ],
   "source": [
    "# Calculating the size of training and testing images\n",
    "train_count = len(glob.glob(train_path + '**/*.jpeg'))\n",
    "test_count = len(glob.glob(test_path + '**/*.jpeg'))\n",
    "print(train_count, test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "variable-marketing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2609"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "peripheral-inquiry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.43475809693336487 Train Accuracy: 0.8465454458339322\n",
      "Epoch: 0 Train Loss: tensor(0.4348) Train Accuracy: 0.8465454458339322 Test Accuracy: 0.9028925619834711\n",
      "Train Loss: 0.3414214253425598 Train Accuracy: 0.8814982511618993\n",
      "Epoch: 1 Train Loss: tensor(0.3414) Train Accuracy: 0.8814982511618993 Test Accuracy: 0.9411157024793388\n",
      "Train Loss: 0.2770894467830658 Train Accuracy: 0.9044846916774472\n",
      "Epoch: 2 Train Loss: tensor(0.2771) Train Accuracy: 0.9044846916774472 Test Accuracy: 0.9597107438016529\n",
      "Train Loss: 0.23658722639083862 Train Accuracy: 0.9195534473671602\n",
      "Epoch: 3 Train Loss: tensor(0.2366) Train Accuracy: 0.9195534473671602 Test Accuracy: 0.9483471074380165\n",
      "Train Loss: 0.2134728580713272 Train Accuracy: 0.9275070672224618\n",
      "Epoch: 4 Train Loss: tensor(0.2135) Train Accuracy: 0.9275070672224618 Test Accuracy: 0.9183884297520661\n",
      "Train Loss: 0.18720652163028717 Train Accuracy: 0.9372095251784773\n",
      "Epoch: 5 Train Loss: tensor(0.1872) Train Accuracy: 0.9372095251784773 Test Accuracy: 0.9576446280991735\n",
      "Train Loss: 0.1719711273908615 Train Accuracy: 0.943330458530976\n",
      "Epoch: 6 Train Loss: tensor(0.1720) Train Accuracy: 0.943330458530976 Test Accuracy: 0.8708677685950413\n",
      "Train Loss: 0.16132009029388428 Train Accuracy: 0.9474270518901825\n",
      "Epoch: 7 Train Loss: tensor(0.1613) Train Accuracy: 0.9474270518901825 Test Accuracy: 0.96900826446281\n",
      "Train Loss: 0.14735372364521027 Train Accuracy: 0.9520506923482344\n",
      "Epoch: 8 Train Loss: tensor(0.1474) Train Accuracy: 0.9520506923482344 Test Accuracy: 0.9731404958677686\n",
      "Train Loss: 0.14163340628147125 Train Accuracy: 0.9545781227540606\n",
      "Epoch: 9 Train Loss: tensor(0.1416) Train Accuracy: 0.9545781227540606 Test Accuracy: 0.9628099173553719\n"
     ]
    }
   ],
   "source": [
    "# Model training and saving best model\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().data * images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))        \n",
    "        \n",
    "    train_accuracy = train_accuracy / train_count\n",
    "    train_loss = train_loss / train_count\n",
    "    print(f'Train Loss: {train_loss} Train Accuracy: {train_accuracy}')\n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy) +' Test Accuracy: '+str(test_accuracy))\n",
    "    \n",
    "    # Save the best model\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
